{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.4.3"
    },
    "colab": {
      "name": "csv_parser.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mat-O-Lab/MSEO/blob/main/tools/csv_parser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T23aj7f1req6"
      },
      "source": [
        "from numpy.core.numeric import NaN\n",
        "#-*- coding: UTF-8 -*-\n",
        "#@title Code - Run Once To Start { vertical-output: true, display-mode: \"form\" }\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd \n",
        "import io\n",
        "import sys\n",
        "import ast, re\n",
        "import base64\n",
        "import json\n",
        "from dateutil.parser import parse\n",
        "from contextlib import redirect_stderr\n",
        "from csv import Sniffer\n",
        "import chardet\n",
        "from urllib.request import urlopen\n",
        "\n",
        "%matplotlib notebook\n",
        "\n",
        "!pip install Owlready2\n",
        "from owlready2 import *\n",
        "\n",
        "#there is a bug in Owlready2 when having imports in turtle in a owl file\n",
        "# if the error is thrown, load again and it is fine\n",
        "try:\n",
        "  mseo=get_ontology(\"https://purl.matolab.org/mseo/mid\").load()\n",
        "except:\n",
        "  mseo=get_ontology(\"https://purl.matolab.org/mseo/mid\").load()\n",
        "  \n",
        "cco_mu=get_ontology(\"http://www.ontologyrepository.com/CommonCoreOntologies/Mid/UnitsOfMeasureOntology/\").load()\n",
        "qudt=get_ontology('http://www.qudt.org/qudt/owl/1.0.0/unit.owl').load()\n",
        "\n",
        "class CSV_Annotator():\n",
        "  def __init__(self, csv_url=''):\n",
        "      self.csv_url = csv_url\n",
        "      self.json_ld_context=[\n",
        "        \"http://www.w3.org/ns/csvw\", {\n",
        "        \"cco\": \"http://www.ontologyrepository.com/CommonCoreOntologies/\",\n",
        "        \"mseo\": mseo.base_iri,\n",
        "        \"label\": \"http://www.w3.org/2000/01/rdf-schema#label\",\n",
        "        \"xsd\": \"http://www.w3.org/2001/XMLSchema#\"}\n",
        "        ]\n",
        "      self.umlaute_dict = {\n",
        "      '\\u00e4': 'ae',  # U+00E4\t   \\xc3\\xa4\n",
        "      '\\u00f6': 'oe',  # U+00F6\t   \\xc3\\xb6\n",
        "      '\\u00fc': 'ue',  # U+00FC\t   \\xc3\\xbc\n",
        "      '\\u00c4': 'Ae',  # U+00C4\t   \\xc3\\x84\n",
        "      '\\u00d6': 'Oe',  # U+00D6\t   \\xc3\\x96\n",
        "      '\\u00dc': 'Ue',  # U+00DC\t   \\xc3\\x9c\n",
        "      '\\u00df': 'ss',  # U+00DF\t   \\xc3\\x9f\n",
        "      }\n",
        "\n",
        "  def _create_initial_widgets(self):\n",
        "      self.url_widget=widgets.Text(\n",
        "          value='',\n",
        "          placeholder='put ur url to a *-metadata.json here',\n",
        "          description='Url:',\n",
        "          disabled=False\n",
        "          )\n",
        "      self.uploader = widgets.FileUpload(accept='',  # Accepted file extension e.g. '.txt', '.pdf', 'image/*', 'image/*,.pdf'\n",
        "                                          multiple=False,  # True to accept multiple files upload else False\n",
        "                                          description='Upload'\n",
        "                                          )\n",
        "      self.clear_button = widgets.Button(description='Clear!', layout=widgets.Layout(width='100px')); \n",
        "      self.file_dialog= widgets.HBox([widgets.Label(value=\"File:\"), self.url_widget ,self.uploader,self.clear_button])\n",
        "      self.clear_button.on_click(self._on_clear)\n",
        "      \n",
        "      self.out = widgets.Output()  # this is the output widget in which the df is displayed\n",
        "      self.encoding = widgets.Dropdown(\n",
        "          options=['auto', 'ISO-8859-1', 'UTF-8', 'ascii', 'latin-1','cp273'],\n",
        "          value='auto',\n",
        "          description='Encoding:',\n",
        "          disabled=False,\n",
        "      )\n",
        "      self.separator = widgets.Dropdown(\n",
        "          options=['auto', ',',';', '\\t', '|', \"\\s+\",\"\\s+|\\t+|\\s+\\t+|\\t+\\s+\"],\n",
        "          value='auto',\n",
        "          description='separator:',\n",
        "          disabled=False,\n",
        "      )\n",
        "      self.settings= widgets.HBox([self.encoding, self.separator])\n",
        "      self.process_button = widgets.Button(description='Process!', layout=widgets.Layout(width='200px')); \n",
        "      self.process_button.on_click(self._on_process)\n",
        "  def _on_clear(self,button):\n",
        "    self.url_widget.value=''\n",
        "    self.uploader.value.clear()\n",
        "    self.uploader._counter = 0\n",
        "\n",
        "  def _on_process(self,button):\n",
        "    with self.out:\n",
        "      clear_output()\n",
        "      if not (self.url_widget.value or self.uploader.value.keys()):\n",
        "          print('pls upload a file first or insert a url')\n",
        "          return\n",
        "      if self.url_widget.value:\n",
        "        self.csv_url=self.url_widget.value\n",
        "        file_name=self.csv_url.split('/')[-1]\n",
        "        #response=requests.get(self.csv_meta_url)\n",
        "        #self.file_data = response.text\n",
        "        self.file_data = urlopen(self.csv_url).read()\n",
        "        #print(self.file_data)\n",
        "      else:\n",
        "        input_file=self.uploader.value[list(self.uploader.value.keys())[0]]\n",
        "        self.csv_meta_url=input_file['metadata']['name']\n",
        "        file_name = input_file['metadata']['name']\n",
        "        self.file_data = input_file['content']\n",
        "      if self.encoding.value=='auto':\n",
        "        self.encoding.value=self.get_encoding(self.file_data)\n",
        "      if self.separator.value=='auto':\n",
        "        try:\n",
        "          self.separator.value=self.get_column_separator(self.file_data)\n",
        "        except:\n",
        "          print('cant find separator, pls manualy select')\n",
        "      metafile_name, result =self.process_file(file_name,self.file_data,self.separator.value,self.encoding.value)\n",
        "      print(result)\n",
        "      res = result\n",
        "      b64 = base64.b64encode(res.encode())\n",
        "      payload = b64.decode()\n",
        "      html_buttons = '''<html>\n",
        "      <head>\n",
        "      <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
        "      </head>\n",
        "      <body>\n",
        "      <a download=\"{filename}\" href=\"data:text/json;base64,{payload}\" download>\n",
        "      <button class=\"p-Widget jupyter-widgets jupyter-button widget-button mod-warning\">Download File</button>\n",
        "      </a>\n",
        "      </body>\n",
        "      </html>\n",
        "      '''\n",
        "      html_button = html_buttons.format(payload=payload,filename=metafile_name)\n",
        "      display(widgets.HTML(html_button))\n",
        "\n",
        "    \n",
        "  def display_widgets(self):\n",
        "    self._create_initial_widgets()\n",
        "    display(widgets.VBox(\n",
        "                [\n",
        "                  self.file_dialog,\n",
        "                  self.settings,\n",
        "                  self.process_button,\n",
        "                  self.out\n",
        "                ]\n",
        "            )\n",
        "    )\n",
        "\n",
        "  def get_encoding(self,file_data):\n",
        "    result = chardet.detect(file_data)\n",
        "    return result['encoding']\n",
        "\n",
        "  def get_column_separator(self,file_data):\n",
        "    file_string = io.StringIO(file_data.decode(self.encoding.value))\n",
        "    #print(file_string) \n",
        "    sniffer = Sniffer()\n",
        "    dialect = sniffer.sniff(file_string.read(512))\n",
        "    return dialect.delimiter\n",
        "\n",
        "  def get_header_lenght(self,file_data, separator_string, encoding):\n",
        "    file_string = io.StringIO(file_data.decode(encoding))  \n",
        "    f = io.StringIO()\n",
        "    with redirect_stderr(f):\n",
        "        df = pd.read_csv(file_string,sep=separator_string,error_bad_lines=False,warn_bad_lines=True,header=None)\n",
        "    f.seek(0)\n",
        "    #without utf string code b' \n",
        "    warn_str=f.read()[2:-2]\n",
        "    warnlist=warn_str.split('\\\\n')[:-1]\n",
        "    #print(warnlist)\n",
        "    # readout row index and column count in warnings\n",
        "    line_numbers=[int(re.search('Skipping line (.+?):', line).group(1)) for line in warnlist]\n",
        "    column_numbers=[int(line[-1]) for line in warnlist]\n",
        "    column_numbersm1=column_numbers.copy()\n",
        "    if not column_numbersm1:\n",
        "      #no additional header\n",
        "      return 0,0\n",
        "    #pop lats element, so column_numbers is always lenght +1\n",
        "    column_numbersm1.pop(-1)\n",
        "    #assumes that the file ends with a uniform table with constant column count\n",
        "    #determine changes in counted columns starting from the last line of file\n",
        "    changed_column_count_line=[line_numbers[index+1] for index in reversed(range(len(column_numbersm1))) if column_numbersm1[index]!=column_numbers[index+1]]\n",
        "    #print(changed_column_count_line)\n",
        "    \n",
        "    if changed_column_count_line:\n",
        "      # additional header has ends in line before the last change of column count\n",
        "      first_head_line=changed_column_count_line[0]-1\n",
        "    elif line_numbers:\n",
        "      first_head_line=line_numbers[0]-1\n",
        "    else:\n",
        "      first_head_line=0\n",
        "    max_columns_additional_header=(max(column_numbers[:line_numbers.index(first_head_line+1)-1]))\n",
        "    return first_head_line, max_columns_additional_header\n",
        "\n",
        "  def get_num_header_rows_and_dataframe(self,file_data,separator_string, header_lenght, encoding):\n",
        "    file_string = io.StringIO(file_data.decode(encoding))\n",
        "    num_header_rows=1\n",
        "    #decimal_delimiter='.'\n",
        "    good_readout=False\n",
        "    while not good_readout:\n",
        "      file_string.seek(0)\n",
        "      table_data = pd.read_csv(file_string,header=list(range(num_header_rows)),sep=separator_string,skiprows=header_lenght,encoding=encoding)    \n",
        "      #test if all text values in first table row -> is a second header row\n",
        "      all_text=all([self.get_value_type(value)=='TEXT' for column,value in table_data.iloc[0].items()])\n",
        "      if all_text:\n",
        "        num_header_rows+=1\n",
        "        continue\n",
        "      else:\n",
        "        good_readout=True\n",
        "    return num_header_rows, table_data\n",
        "\n",
        "  def get_unit(self,string):\n",
        "    found=list(cco_mu.search(alternative_label=string))\\\n",
        "            +list(cco_mu.search(SI_unit_symbol=string))\\\n",
        "            +list(mseo.search(alternative_label=string))\\\n",
        "            +list(mseo.search(SI_unit_symbol=string))\\\n",
        "            +list(qudt.search(symbol=string))\\\n",
        "            +list(qudt.search(abbreviation=string))\\\n",
        "            +list(qudt.search(ucumCode=string))\n",
        "    if found:\n",
        "      return {\"cco:uses_measurement_unit\": {\"@id\": str(found[0].iri), \"@type\": str(found[0].is_a)}}\n",
        "    else:\n",
        "      return {}\n",
        "\n",
        "  def is_date(self,string, fuzzy=False):\n",
        "      try: \n",
        "          parse(string, fuzzy=fuzzy)\n",
        "          return True\n",
        "\n",
        "      except ValueError:\n",
        "          return False\n",
        "\n",
        "  def get_value_type(self,string):\n",
        "      string=str(string)\n",
        "      #remove spaces and replace , with . and\n",
        "      string=string.strip().replace(',','.')\n",
        "      if len(string) == 0: return 'BLANK'\n",
        "      try:\n",
        "          t=ast.literal_eval(string)\n",
        "      except ValueError:\n",
        "          return 'TEXT'\n",
        "      except SyntaxError:\n",
        "          if self.is_date(string):\n",
        "            return 'DATE'\n",
        "          else:\n",
        "            return 'TEXT'\n",
        "      else:\n",
        "          if type(t) in [int, float, bool]:\n",
        "            if type(t) is int:\n",
        "                return 'INT'\n",
        "            if t in set((True,False)):\n",
        "                return 'BOOL'\n",
        "            if type(t) is float:\n",
        "                return 'FLOAT'\n",
        "          else:\n",
        "              return 'TEXT' \n",
        "\n",
        "  def describe_value(self,value_string):\n",
        "    if pd.isna(value_string):\n",
        "      return {}\n",
        "    elif self.get_value_type(value_string)=='INT':\n",
        "      return {'cco:has_integer_value': {'@value':value_string, '@type': 'xsd:integer'}}\n",
        "    elif self.get_value_type(value_string)=='BOOL':\n",
        "      return {'cco:has_bolean_value': {'@value':value_string, '@type': 'xsd:boolean'}}\n",
        "    elif self.get_value_type(value_string)=='FLOAT':\n",
        "      return {'cco:has_decimal_value': {'@value':value_string, '@type': 'xsd:decimal'}}\n",
        "    elif self.get_value_type(value_string)=='DATE':\n",
        "      return {'cco:has_datetime_value': {'@value':str(parse(value_string)), '@type': 'xsd:dateTime'}}\n",
        "    else:\n",
        "      # check if its a unit\n",
        "      unit_dict=self.get_unit(value_string)\n",
        "      if unit_dict:\n",
        "        return unit_dict\n",
        "      else:\n",
        "        return {'cco:has_text_value': {'@value':value_string, '@type': 'xsd:string'}}\n",
        "\n",
        "  def make_id(self,string,namespace=None):\n",
        "    for k in self.umlaute_dict.keys():\n",
        "          string = string.replace(k, self.umlaute_dict[k])\n",
        "    if namespace:\n",
        "      return namespace+':'+re.sub('[^A-ZÜÖÄa-z0-9]+', '', string.title().replace(\" \", \"\"))\n",
        "    else:\n",
        "      return './'+re.sub('[^A-ZÜÖÄa-z0-9]+', '', string.title().replace(\" \", \"\"))\n",
        "\n",
        "  def get_additional_header(self,file_data,separator,encoding):\n",
        "    # get lenght of additional header\n",
        "    header_lenght, max_columns_additional_header=self.get_header_lenght(file_data,separator,encoding)\n",
        "    #print(header_lenght)\n",
        "    if header_lenght:\n",
        "      #print(header_lenght,max_columns_additional_header)\n",
        "      file_string = io.StringIO(file_data.decode(encoding))\n",
        "      header_data = pd.read_csv(file_string,header=None,sep=separator,nrows=header_lenght,names=range(max_columns_additional_header),encoding=encoding,skip_blank_lines=False)\n",
        "      header_data['row']=header_data.index\n",
        "      #header_data.dropna(how='all', inplace=True)\n",
        "      header_data.rename(columns={0: 'param'}, inplace=True)\n",
        "      header_data.set_index('param',inplace=True)\n",
        "      header_data=header_data[~header_data.index.duplicated()]\n",
        "      header_data.dropna(thresh=2, inplace=True)\n",
        "      return header_data, header_lenght\n",
        "    else:\n",
        "      return None, 0\n",
        "\n",
        "\n",
        "  def serialize_header(self,header_data,file_namespace=None):\n",
        "    params=list()\n",
        "    info_line_iri=\"cco:InformationLine\"\n",
        "    for parm_name, data in header_data.to_dict(orient='index').items():\n",
        "      #describe_value(data['value'])\n",
        "      para_dict={'@id': self.make_id(parm_name,file_namespace),'label':parm_name,'@type': info_line_iri}\n",
        "      for col_name, value in data.items():\n",
        "        #print(parm_name,col_name, value)\n",
        "        if col_name=='row':\n",
        "          para_dict['mseo:has_row_index']={\"@value\": data['row'],\"@type\": \"xsd:integer\"}\n",
        "        else:\n",
        "          para_dict={**para_dict,**self.describe_value(value)}\n",
        "      params.append(para_dict)\n",
        "    #print(params)\n",
        "    return params\n",
        "    \n",
        "\n",
        "  def process_file(self,file_name,file_data,separator,encoding):\n",
        "    #init results dict\n",
        "    data_root_url=\"https://github.com/Mat-O-Lab/resources/\"\n",
        "    #file_namespace=data_root_url+file_name.split('.')[0]\n",
        "    file_namespace=None\n",
        "    metadata_csvw = dict()\n",
        "    metadata_csvw[\"@context\"]=self.json_ld_context\n",
        "    #metadata_csvw[\"@id\"]=file_namespace\n",
        "    metadata_csvw[\"url\"]=file_name\n",
        "    # read additional header lines and provide as meta in results dict\n",
        "    header_data, header_lenght=self.get_additional_header(file_data,separator,encoding)\n",
        "    #print(header_lenght)\n",
        "    #metadata_csvw[\"params\"]=header_data.dropna().to_dict(orient='index')\n",
        "    if header_lenght:\n",
        "      #print(\"serialze additinal header\")\n",
        "      metadata_csvw[\"notes\"]=self.serialize_header(header_data,file_namespace)\n",
        "    # read tabular data structure, and determine number of header lines for column description used\n",
        "    #print(get_num_header_rows_and_dataframe(file_data,separator,header_lenght,encoding))\n",
        "    #print(header_lenght)\n",
        "    header_lines, table_data=self.get_num_header_rows_and_dataframe(file_data,separator,header_lenght,encoding)\n",
        "    # describe dialect\n",
        "    metadata_csvw[\"dialect\"]={\"delimiter\": separator,\n",
        "    \"skipRows\": header_lenght, \"headerRowCount\": header_lines, \"encoding\": encoding}\n",
        "    # describe columns\n",
        "    if header_lines==1:\n",
        "      # see if there might be a unit string at the end of each title\n",
        "      column_json=list()\n",
        "      for index, title in enumerate(table_data.columns):\n",
        "        if len(title.split(' '))>1:\n",
        "          unit_json=self.get_unit(title.split(' ')[-1])  \n",
        "        else:\n",
        "          unit_json={}\n",
        "        json_str={**{'titles': title,'@id': self.make_id(title), \"@type\": \"Column\"},**unit_json}\n",
        "        column_json.append(json_str)\n",
        "      metadata_csvw[\"tableSchema\"]={\"columns\":column_json}\n",
        "      #metadata_csvw[\"tableSchema\"]={\"columns\":list({'titles':column, '@id': make_id(column), \"@type\": \"Column\"} for column in table_data.columns)}\n",
        "    else:\n",
        "      column_json=list()\n",
        "      for index, (title,unit_str) in enumerate(table_data.columns):\n",
        "        json_str={**{'titles': title,'@id': self.make_id(title), \"@type\": \"Column\"},**self.get_unit(unit_str)}\n",
        "        #print(json_str)\n",
        "        column_json.append(json_str)\n",
        "      metadata_csvw[\"tableSchema\"]={\"columns\":column_json}\n",
        "    result=json.dumps(metadata_csvw, indent = 4)\n",
        "    meta_file_name = file_name.split(sep='.')[0] + '-metadata.json'\n",
        "    return meta_file_name, result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtuhMq9VzdvR",
        "cellView": "form"
      },
      "source": [
        "#@title Dialog - Run Cell to begin\n",
        "dialog = CSV_Annotator()\n",
        "dialog.display_widgets()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
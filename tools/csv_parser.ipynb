{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.4.3"
    },
    "colab": {
      "name": "csv_parser.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mat-O-Lab/MSEO/blob/main/tools/csv_parser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T23aj7f1req6"
      },
      "source": [
        "from numpy.core.numeric import NaN\n",
        "#-*- coding: UTF-8 -*-\n",
        "#@title Code { vertical-output: true, display-mode: \"form\" }\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd \n",
        "import io\n",
        "import sys\n",
        "import ast, re\n",
        "import base64\n",
        "import json\n",
        "from dateutil.parser import parse\n",
        "from contextlib import redirect_stderr\n",
        "from csv import Sniffer\n",
        "import chardet \n",
        "%matplotlib notebook\n",
        "\n",
        "!pip install Owlready2\n",
        "from owlready2 import *\n",
        "\n",
        "#there is a bug in Owlready2 when having imports in turtle in a owl file\n",
        "# if the error is thrown, load again and it is fine\n",
        "try:\n",
        "  mseo=get_ontology(\"https://purl.matolab.org/mseo/mid\").load()\n",
        "except:\n",
        "  mseo=get_ontology(\"https://purl.matolab.org/mseo/mid\").load()\n",
        "  \n",
        "cco_mu=get_ontology(\"http://www.ontologyrepository.com/CommonCoreOntologies/Mid/UnitsOfMeasureOntology/\").load()\n",
        "qudt=get_ontology('http://www.qudt.org/qudt/owl/1.0.0/unit.owl').load()\n",
        "\n",
        "json_ld_context=[\"http://www.w3.org/ns/csvw\", {\n",
        "    \"cco\": \"http://www.ontologyrepository.com/CommonCoreOntologies/\",\n",
        "    \"mseo\": mseo.base_iri,\n",
        "    \"xsd\": \"http://www.w3.org/2001/XMLSchema#\"}\n",
        "    ]\n",
        "\n",
        "def get_encoding(file_data):\n",
        "  result = chardet.detect(file_data)\n",
        "  return result['encoding']\n",
        "\n",
        "def get_column_separator(file_data):\n",
        "  file_string = io.StringIO(file_data.decode(encoding.value)) \n",
        "  sniffer = Sniffer()\n",
        "  dialect = sniffer.sniff(file_string.read(512))\n",
        "  return dialect.delimiter\n",
        "\n",
        "def get_header_lenght(file_data, separator_string, encoding):\n",
        "  file_string = io.StringIO(file_data.decode(encoding))  \n",
        "  f = io.StringIO()\n",
        "  with redirect_stderr(f):\n",
        "      df = pd.read_csv(file_string,sep=separator.value,error_bad_lines=False,warn_bad_lines=True,header=None)\n",
        "  f.seek(0)\n",
        "  #without utf string code b' \n",
        "  warn_str=f.read()[2:-2]\n",
        "  warnlist=warn_str.split('\\\\n')[:-1]\n",
        "  #print(warnlist)\n",
        "  # readout row index and column count in warnings\n",
        "  line_numbers=[int(re.search('Skipping line (.+?):', line).group(1)) for line in warnlist]\n",
        "  column_numbers=[int(line[-1]) for line in warnlist]\n",
        "  column_numbersm1=column_numbers.copy()\n",
        "  if not column_numbersm1:\n",
        "    #no additional header\n",
        "    return 0,0\n",
        "  #pop lats element, so column_numbers is always lenght +1\n",
        "  column_numbersm1.pop(-1)\n",
        "  #assumes that the file ends with a uniform table with constant column count\n",
        "  #determine changes in counted columns starting from the last line of file\n",
        "  changed_column_count_line=[line_numbers[index+1] for index in reversed(range(len(column_numbersm1))) if column_numbersm1[index]!=column_numbers[index+1]]\n",
        "  #print(changed_column_count_line)\n",
        "  \n",
        "  if changed_column_count_line:\n",
        "    # additional header has ends in line before the last change of column count\n",
        "    first_head_line=changed_column_count_line[0]-1\n",
        "  elif line_numbers:\n",
        "    first_head_line=line_numbers[0]-1\n",
        "  else:\n",
        "    first_head_line=0\n",
        "  #print(line_numbers)\n",
        "  #print(column_numbers)\n",
        "  #print(line_numbers.index(first_head_line))\n",
        "  max_columns_additional_header=(max(column_numbers[:line_numbers.index(first_head_line+1)-1]))\n",
        "  return first_head_line, max_columns_additional_header\n",
        "\n",
        "  # except:\n",
        "  #   e = sys.exc_info()[1]\n",
        "  #   if 'Error tokenizing' in e.args[0]:\n",
        "  #     #example Error tokenizing data. C error: Expected 3 fields in line 17, saw 5\n",
        "  #     # column header will be at line 17\n",
        "  #     line = int(re.search('fields in line (.+?),', e.args[0]).group(1))-1\n",
        "  #     return line\n",
        "  # # return zero if ther is no error -> no additional header\n",
        "  #return 0\n",
        "\n",
        "def get_num_header_rows_and_dataframe(file_data,separator_string, header_lenght, encoding):\n",
        "  file_string = io.StringIO(file_data.decode(encoding))\n",
        "  num_header_rows=1\n",
        "  #decimal_delimiter='.'\n",
        "  good_readout=False\n",
        "  while not good_readout:\n",
        "    file_string.seek(0)\n",
        "    #print(num_header_rows,decimal_delimiter)\n",
        "    #table_data = pd.read_csv(file_string,decimal=decimal_delimiter,header=list(range(num_header_rows)),sep=separator_string,skiprows=header_lenght,encoding=encoding)\n",
        "    table_data = pd.read_csv(file_string,header=list(range(num_header_rows)),sep=separator_string,skiprows=header_lenght,encoding=encoding)    \n",
        "    #test if all text values in first table row -> is a second header row\n",
        "    all_text=all([get_value_type(value)=='TEXT' for column,value in table_data.iloc[0].items()])\n",
        "    if all_text:\n",
        "      #print('All cells of first datarow are of type text!')\n",
        "      num_header_rows+=1\n",
        "      continue\n",
        "    else:\n",
        "      #print('first data row datatypes')\n",
        "      #print([get_value_type(value) for column,value in table_data.iloc[1].items()])\n",
        "      good_readout=True\n",
        "  return num_header_rows, table_data\n",
        "\n",
        "def get_unit(string):\n",
        "  found=list(cco_mu.search(alternative_label=string))\\\n",
        "          +list(cco_mu.search(SI_unit_symbol=string))\\\n",
        "          +list(mseo.search(alternative_label=string))\\\n",
        "          +list(mseo.search(SI_unit_symbol=string))\\\n",
        "          +list(qudt.search(symbol=string))\\\n",
        "          +list(qudt.search(abbreviation=string))\\\n",
        "          +list(qudt.search(ucumCode=string))\n",
        "  if found:\n",
        "    return {\"cco:uses_measurement_unit\": {\"@id\": str(found[0].iri), \"@type\": str(found[0].is_a)}}\n",
        "  else:\n",
        "    return {}\n",
        "\n",
        "def is_date(string, fuzzy=False):\n",
        "    try: \n",
        "        parse(string, fuzzy=fuzzy)\n",
        "        return True\n",
        "\n",
        "    except ValueError:\n",
        "        return False\n",
        "\n",
        "def get_value_type(string):\n",
        "    string=str(string)\n",
        "    #remove spaces and replace , with . and\n",
        "    string=string.strip().replace(',','.')\n",
        "    if len(string) == 0: return 'BLANK'\n",
        "    try:\n",
        "        t=ast.literal_eval(string)\n",
        "    except ValueError:\n",
        "        return 'TEXT'\n",
        "    except SyntaxError:\n",
        "        if is_date(string):\n",
        "          return 'DATE'\n",
        "        else:\n",
        "          return 'TEXT'\n",
        "    else:\n",
        "        if type(t) in [int, float, bool]:\n",
        "          if type(t) is int:\n",
        "              return 'INT'\n",
        "          if t in set((True,False)):\n",
        "              return 'BOOL'\n",
        "          if type(t) is float:\n",
        "              return 'FLOAT'\n",
        "        else:\n",
        "            return 'TEXT' \n",
        "\n",
        "def describe_value(value_string):\n",
        "  if pd.isna(value_string):\n",
        "    return {}\n",
        "  elif get_value_type(value_string)=='INT':\n",
        "    return {'cco:has_integer_value': {'@value':value_string, '@type': 'xsd:integer'}}\n",
        "  elif get_value_type(value_string)=='BOOL':\n",
        "    return {'cco:has_bolean_value': {'@value':value_string, '@type': 'xsd:boolean'}}\n",
        "  elif get_value_type(value_string)=='FLOAT':\n",
        "    return {'cco:has_decimal_value': {'@value':value_string, '@type': 'xsd:decimal'}}\n",
        "  elif get_value_type(value_string)=='DATE':\n",
        "    return {'cco:has_datetime_value': {'@value':str(parse(value_string)), '@type': 'xsd:dateTime'}}\n",
        "  else:\n",
        "    # check if its a unit\n",
        "    unit_dict=get_unit(value_string)\n",
        "    if unit_dict:\n",
        "      return unit_dict\n",
        "    else:\n",
        "      return {'cco:has_text_value': {'@value':value_string, '@type': 'xsd:string'}}\n",
        "\n",
        "umlaute_dict = {\n",
        "    '\\u00e4': 'ae',  # U+00E4\t   \\xc3\\xa4\n",
        "    '\\u00f6': 'oe',  # U+00F6\t   \\xc3\\xb6\n",
        "    '\\u00fc': 'ue',  # U+00FC\t   \\xc3\\xbc\n",
        "    '\\u00c4': 'Ae',  # U+00C4\t   \\xc3\\x84\n",
        "    '\\u00d6': 'Oe',  # U+00D6\t   \\xc3\\x96\n",
        "    '\\u00dc': 'Ue',  # U+00DC\t   \\xc3\\x9c\n",
        "    '\\u00df': 'ss',  # U+00DF\t   \\xc3\\x9f\n",
        "}\n",
        "\n",
        "def make_id(string,namespace=None):\n",
        "  for k in umlaute_dict.keys():\n",
        "        string = string.replace(k, umlaute_dict[k])\n",
        "  if namespace:\n",
        "    return namespace+':'+re.sub('[^A-ZÜÖÄa-z0-9]+', '', string.title().replace(\" \", \"\"))\n",
        "  else:\n",
        "    return './'+re.sub('[^A-ZÜÖÄa-z0-9]+', '', string.title().replace(\" \", \"\"))\n",
        "\n",
        "def get_additional_header(file_data,separator,encoding):\n",
        "  # get lenght of additional header\n",
        "  header_lenght, max_columns_additional_header=get_header_lenght(file_data,separator,encoding)\n",
        "  if header_lenght:\n",
        "    #print(header_lenght,max_columns_additional_header)\n",
        "    file_string = io.StringIO(file_data.decode(encoding))\n",
        "    header_data = pd.read_csv(file_string,header=None,sep=separator,nrows=header_lenght,names=range(max_columns_additional_header),encoding=encoding,skip_blank_lines=False)\n",
        "    header_data['row']=header_data.index\n",
        "    #header_data.dropna(how='all', inplace=True)\n",
        "    header_data.rename(columns={0: 'param'}, inplace=True)\n",
        "    header_data.set_index('param',inplace=True)\n",
        "    header_data=header_data[~header_data.index.duplicated()]\n",
        "    header_data.dropna(thresh=2, inplace=True)\n",
        "    return header_data, header_lenght\n",
        "  else:\n",
        "    return None, 0\n",
        "\n",
        "\n",
        "def serialize_header(header_data,file_namespace=None):\n",
        "  params=list()\n",
        "  info_line_iri=\"cco:InformationLine\"\n",
        "  for parm_name, data in header_data.to_dict(orient='index').items():\n",
        "    #describe_value(data['value'])\n",
        "    para_dict={'@id': make_id(parm_name,file_namespace),'label':parm_name,'@type': info_line_iri}\n",
        "    for col_name, value in data.items():\n",
        "      #print(parm_name,col_name, value)\n",
        "      if col_name=='row':\n",
        "        para_dict['mseo:has_row_index']={\"@value\": data['row'],\"@type\": \"xsd:integer\"}\n",
        "      else:\n",
        "        para_dict={**para_dict,**describe_value(value)}\n",
        "    params.append(para_dict)\n",
        "  #print(params)\n",
        "  return params\n",
        "  \n",
        "\n",
        "def process_file(file_name,file_data,separator,encoding):\n",
        "  #init results dict\n",
        "  data_root_url=\"https://github.com/Mat-O-Lab/resources/\"\n",
        "  #file_namespace=data_root_url+file_name.split('.')[0]\n",
        "  file_namespace=None\n",
        "  metadata_csvw = dict()\n",
        "  metadata_csvw[\"@context\"]=json_ld_context\n",
        "  #metadata_csvw[\"@id\"]=file_namespace\n",
        "  metadata_csvw[\"url\"]=file_name\n",
        "  # read additional header lines and provide as meta in results dict\n",
        "  header_data, header_lenght=get_additional_header(file_data,separator,encoding)\n",
        "  #print(header_lenght)\n",
        "  #metadata_csvw[\"params\"]=header_data.dropna().to_dict(orient='index')\n",
        "  if header_lenght:\n",
        "    #print(\"serialze additinal header\")\n",
        "    metadata_csvw[\"notes\"]=serialize_header(header_data,file_namespace)\n",
        "  # read tabular data structure, and determine number of header lines for column description used\n",
        "  #print(get_num_header_rows_and_dataframe(file_data,separator,header_lenght,encoding))\n",
        "  #print(header_lenght)\n",
        "  header_lines, table_data=get_num_header_rows_and_dataframe(file_data,separator,header_lenght,encoding)\n",
        "  # describe dialect\n",
        "  metadata_csvw[\"dialect\"]={\"delimiter\": separator,\n",
        "  \"skipRows\": header_lenght, \"headerRowCount\": header_lines, \"encoding\": encoding}\n",
        "  # describe columns\n",
        "  if header_lines==1:\n",
        "    # see if there might be a unit string at the end of each title\n",
        "    column_json=list()\n",
        "    for index, title in enumerate(table_data.columns):\n",
        "      if len(title.split(' '))>1:\n",
        "        unit_json=get_unit(title.split(' ')[-1])  \n",
        "      else:\n",
        "        unit_json={}\n",
        "      json_str={**{'titles': title,'@id': make_id(title), \"@type\": \"Column\"},**unit_json}\n",
        "      column_json.append(json_str)\n",
        "    metadata_csvw[\"tableSchema\"]={\"columns\":column_json}\n",
        "    #metadata_csvw[\"tableSchema\"]={\"columns\":list({'titles':column, '@id': make_id(column), \"@type\": \"Column\"} for column in table_data.columns)}\n",
        "  else:\n",
        "    column_json=list()\n",
        "    for index, (title,unit_str) in enumerate(table_data.columns):\n",
        "      json_str={**{'titles': title,'@id': make_id(title), \"@type\": \"Column\"},**get_unit(unit_str)}\n",
        "      #print(json_str)\n",
        "      column_json.append(json_str)\n",
        "    metadata_csvw[\"tableSchema\"]={\"columns\":column_json}\n",
        "  result=json.dumps(metadata_csvw, indent = 4)\n",
        "  meta_file_name = file_name.split(sep='.')[0] + '-metadata.json'\n",
        "  return meta_file_name, result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2XGNfLRQ6Ai",
        "cellView": "form"
      },
      "source": [
        "#@title Dialog { vertical-output: true }\n",
        "# dialog\n",
        "uploader = widgets.FileUpload(\n",
        "    accept='',  # Accepted file extension e.g. '.txt', '.pdf', 'image/*', 'image/*,.pdf'\n",
        "    multiple=False,  # True to accept multiple files upload else False\n",
        "    description='Upload'\n",
        "\n",
        ")\n",
        "clear = widgets.Button(description='Clear!', layout=widgets.Layout(width='100px')); \n",
        "def on_clear(_):\n",
        "  uploader.value.clear()\n",
        "  uploader._counter = 0\n",
        "clear.on_click(on_clear)\n",
        "\n",
        "file= widgets.HBox([widgets.Label(value=\"File:\"), uploader,clear])\n",
        "encoding = widgets.Dropdown(\n",
        "    options=['auto', 'ISO-8859-1', 'UTF-8', 'ascii', 'latin-1','cp273'],\n",
        "    value='auto',\n",
        "    description='Encoding:',\n",
        "    disabled=False,\n",
        ")\n",
        "separator = widgets.Dropdown(\n",
        "    options=['auto', ',',';', '\\t', '|', \"\\s+\",\"\\s+|\\t+|\\s+\\t+|\\t+\\s+\"],\n",
        "    value='auto',\n",
        "    description='separator:',\n",
        "    disabled=False,\n",
        ")\n",
        "settings= widgets.HBox([encoding, separator])\n",
        "button = widgets.Button(description='Process!', layout=widgets.Layout(width='200px')); \n",
        "out = widgets.Output()\n",
        "\n",
        "def on_button_clicked(_):\n",
        "  # \"linking function with output\"\n",
        "  with out:\n",
        "  # what happens when we press the button\n",
        "    clear_output()\n",
        "    if not uploader.value.keys():\n",
        "      print('pls upload a file first')\n",
        "      return\n",
        "    input_file=uploader.value[list(uploader.value.keys())[0]]\n",
        "    file_name = input_file['metadata']['name']\n",
        "    file_data = input_file['content']\n",
        "    if encoding.value=='auto':\n",
        "      encoding.value=get_encoding(file_data)\n",
        "    if separator.value=='auto':\n",
        "      try:\n",
        "        separator.value=get_column_separator(file_data)\n",
        "      except:\n",
        "        print('cant find separator, pls manualy select')\n",
        "    metafile_name, result =process_file(file_name,file_data,separator.value,encoding.value)\n",
        "    print(result)\n",
        "    res = result\n",
        "    b64 = base64.b64encode(res.encode())\n",
        "    payload = b64.decode()\n",
        "    html_buttons = '''<html>\n",
        "    <head>\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
        "    </head>\n",
        "    <body>\n",
        "    <a download=\"{filename}\" href=\"data:text/json;base64,{payload}\" download>\n",
        "    <button class=\"p-Widget jupyter-widgets jupyter-button widget-button mod-warning\">Download File</button>\n",
        "    </a>\n",
        "    </body>\n",
        "    </html>\n",
        "    '''\n",
        "    html_button = html_buttons.format(payload=payload,filename=metafile_name)\n",
        "    display(widgets.HTML(html_button))\n",
        "\n",
        "button.on_click(on_button_clicked)\n",
        "process = widgets.VBox([button,out])\n",
        "display(file,settings,process)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}